{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxA59YkI1Y7Y"
   },
   "source": [
    "# Lab2- Point-of-Interest Recommendation Systems\n",
    "## Urban Computing course - Leiden University\n",
    "#### Instructor: Dr. Mitra Baratchi\n",
    "#### Contributor: Hossein A. Rahmani\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0VWXtk2kFXw"
   },
   "source": [
    "## Outline\n",
    "\n",
    "1.   Recommender systems and Context information\n",
    "2.   General Structure of Location-Based Social Network's Dataset\n",
    "3.   Loading the dataset (Foursquare)\n",
    "4.   Data Pre-processing\n",
    "5.   Matrix Factorization\n",
    "6.   Model Evaluation\n",
    "7.   Utilizing Contextual Information: Geographical Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjNJ7x2IGqma"
   },
   "source": [
    "## Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lsl9EwelGuVm"
   },
   "source": [
    "A recommender system is a simple algorithm with the aim to provide the most relevant recommendation to a user by discovering patterns in a dataset. Recommender systems are a subclass of **information filtering system** addressing the problem of infromation overload. One of the most important applications of recommender systems is in Location-Based Social Networks (LBSNs) to recommend unvisited location to the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W34USwn7LIP5"
   },
   "source": [
    "### Recommender system approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_0yBRgyLL9L"
   },
   "source": [
    "Recommender System approaches are generaly divided into three categories:\n",
    "\n",
    "- **Collaborative Filtering (CF)**: CF is a method of recommending items to users using their past behaviors or ratings as well as similar decisions by other users to predict which items might be appealing to the original customers. The CF method divides into two subcategories, Memory-based and Model-based. In memory-based we use the whole of the user-item matrix to make the recommendation but in model-based we make use of machine learning approaches that are scalable and more efficient.\n",
    "\n",
    "- **Content Based (CB)**: CB suggests items to users by using the characteristics of an item in order to recommend additional items with similar properties.\n",
    "\n",
    "- **Hybrid Method**: A number of applications combine the CF and the CB algorithms. These overcome the limitations of native CF and CB approaches and improve prediction performance. Importantly, they overcome the CF problems such as sparsity and loss of information. \n",
    "\n",
    "Collaborative filtering is also much more popular for web-based recommendations where the data is sparse, i.e., where there is a limited number of reviews by each user or for a particular item.\n",
    "\n",
    "More information for study and comparsion can be find in the [Basics of Recommender Systems: Study of Location-Based Social Networks](https://github.com/rahmanidashti/LRSbasics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFi85yAWJMIb"
   },
   "source": [
    "### Context-Aware Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SckvfegaJRfs"
   },
   "source": [
    "Considering contextual inforamtion into the recommendation process is an important requirement for many applications. For example, through considering the temporal context, a travel recommender system would provide a different vacation recommendation in the winter than what it would recommend in summer. Geographical influence is an important contextual factor that distinguishes the location recommendation from traditional item recommendation, because the check-in behavior depends on locationsâ€™ geographical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kE-1Rm4mcfHs"
   },
   "source": [
    "## LBSN's Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YR1vnRWCbX9X"
   },
   "source": [
    "The dataset of a LBSN is generated through providing a feature for collecting __Check-in__ records. When a user checks in a location, the presence of the user in that location is registered. Generally, a check-in record includes the following data:\n",
    "\n",
    "*   UserID\n",
    "*   LocationID\n",
    "*   Latitude and longitude of the location\n",
    "*   Check-in time\n",
    "\n",
    "For example, a check-in record is something that looks like: **user_0, location_0, 1.372494548, 103.893714, 28-01-2019 14:17:15**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bB2JY4SpbPYq"
   },
   "source": [
    "## Import the required module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FM1lNoUdjIzo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import scipy.sparse as sparse\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11ln990OclFB"
   },
   "source": [
    "## Load the dataset\n",
    "We have uploaded a preprocessed dataset of Gowalla LBSN into a Github repository. We have four files that are needed for this assignment: **size, train, test, and geo**. The _size_ file includes the number of users and locations (or POIs), _train and test_ files stores the user's check-ins and the _geo_ file includes the geograpical information of each location. As mentioned above, we load the dataset from github. The version used here is a small version of Gowalla dataset. You can find other verions in https://github.com/rahmanidashti/LBSNDatasets/ to evaluate the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7JHBpCu1pqT"
   },
   "outputs": [],
   "source": [
    "size = \"https://raw.githubusercontent.com/rahmanidashti/LBSNDatasets/master/Gowalla/Gowalla_2/Gowalla_data_size.txt\"\n",
    "train = \"https://raw.githubusercontent.com/rahmanidashti/LBSNDatasets/master/Gowalla/Gowalla_2/Gowalla_train.txt\"\n",
    "test = \"https://raw.githubusercontent.com/rahmanidashti/LBSNDatasets/master/Gowalla/Gowalla_2/Gowalla_test.txt\"\n",
    "geo = \"https://raw.githubusercontent.com/rahmanidashti/LBSNDatasets/master/Gowalla/Gowalla_2/Gowalla_poi_coos.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9zzCJTAM8Pu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numebr of users is 100 and the number of locations in this dataset is 6436\n"
     ]
    }
   ],
   "source": [
    "size_header = ['users', 'locations']\n",
    "size_data = pd.read_csv(size, sep='\t', names=size_header)\n",
    "for index, eachline in size_data.iterrows():\n",
    "  n_users, n_locations = eachline['users'], eachline['locations']\n",
    "print(\"The numebr of users is\", n_users,\"and the number of locations in this dataset is\", n_locations)\n",
    "\n",
    "header = ['uid', 'lid', 'freq']\n",
    "train_data = pd.read_csv(train, sep='\t', names=header)\n",
    "test_data = pd.read_csv(test, sep='\t', names=header)\n",
    "\n",
    "geo_header = ['lid', 'lat', 'lng']\n",
    "poi_data = pd.read_csv(geo, sep='\t', names=geo_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwnLMUcudlsJ"
   },
   "source": [
    "### Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ew2y3joTkyNf"
   },
   "outputs": [],
   "source": [
    "def read_training_data():\n",
    "  training_matrix = np.zeros((n_users, n_locations))\n",
    "  sparse_training_matrix = sparse.dok_matrix((n_users, n_locations))\n",
    "  for index, row in train_data.iterrows():\n",
    "    uid, lid, freq = row['uid'], row['lid'], row['freq']\n",
    "    uid, lid, freq = int(uid), int(lid), int(freq)\n",
    "    training_matrix[uid, lid] = 1.0\n",
    "    sparse_training_matrix[uid, lid] = freq\n",
    "  return sparse_training_matrix, training_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubiIO-5keMNh"
   },
   "source": [
    "### Load the latitude and longitude of POIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0TcJzxUu9hQ"
   },
   "outputs": [],
   "source": [
    "def read_poi_coos():\n",
    "  poi_coos = {}\n",
    "  for index, row in poi_data.iterrows():\n",
    "    lid, lat, lng = row['lid'], row['lat'], row['lng']\n",
    "    lid, lat, lng = int(lid), float(lat), float(lng)\n",
    "    poi_coos[lid] = (lat, lng)\n",
    "  return poi_coos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sojWEvNpefa9"
   },
   "source": [
    "### Load the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OMM-Z88f4hd8"
   },
   "source": [
    "A part of dataset should be reserved as test data to evalute the model performance. Here, we cosider 20% of the recent check-ins to compose a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUj12Sn4vJF5"
   },
   "outputs": [],
   "source": [
    "def read_ground_truth():\n",
    "  ground_truth = defaultdict(set)\n",
    "  for index, row in test_data.iterrows():\n",
    "    uid, lid, _ = row['uid'], row['lid'], row['freq']\n",
    "    uid, lid = int(uid), int(lid)\n",
    "    ground_truth[uid].add(lid)\n",
    "  return ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DOPcN2d9fsW"
   },
   "source": [
    "### Intialize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvoTaxNw9lw-"
   },
   "outputs": [],
   "source": [
    "sparse_training_matrix, training_matrix = read_training_data()\n",
    "ground_truth = read_ground_truth()\n",
    "poi_coos = read_poi_coos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZRi3l9UCpKp"
   },
   "source": [
    "## Collaborative Filtering\n",
    "As mentioned earlier, one of the most popular approaches in recommender systems is __Collaborative Filtering (CF)__. Initially, the main approach of CF used to be __user-based CF__ . After 2009, with the Netflix chanllenge __Matrix Factorization (MF)__, as a __Model-based CF__ approach has become more popular. The user-based approach seems to perfrom well when the dataset is small but MF shows more scalability with respect to the size of the dataset. In this lab, we will first consider and implement a user-based CF to recommend locations to users, then we proceed with implementing a MF-based approach. Finally, we show the impact of context in RS and integrate one of the most important contextual factors (geographic effect) in location recommendation to the MF model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySj5_KG3erQ6"
   },
   "source": [
    "### User-Based Collaborative Filtering [1]\n",
    " A user-based system can use machine-learning algorithms to group all users who have shown that they have the same taste. The system builds neighborhoods of users who have similar profiles, purchase patterns, or rating patterns. If a person in a neighborhood buys and likes an item, the recommender system can recommend that item to everyone else in the neighborhood. Here, we used __cosine-based similarity__. The cosine-based approach defines the cosine-similarity between two users $x$ and $y$ as:\n",
    " \n",
    " $\\operatorname {sim} (x,y)=\\cos({\\vec {x}},{\\vec {y}})={\\frac {{\\vec {x}}\\cdot {\\vec {y}}}{||{\\vec {x}}||\\times ||{\\vec {y}}||}}={\\frac {\\sum \\limits _{i\\in I_{xy}}r_{x,i}r_{y,i}}{{\\sqrt {\\sum \\limits _{i\\in I_{x}}r_{x,i}^{2}}}{\\sqrt {\\sum \\limits _{i\\in I_{y}}r_{y,i}^{2}}}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMg-KFROzxOL"
   },
   "outputs": [],
   "source": [
    "class UserBasedCF(object):\n",
    "  def __init__(self):\n",
    "    self.rec_score = None\n",
    "    self.epsilon = 1e-9\n",
    "    \n",
    "  def pre_compute_rec_scores(self, C):\n",
    "    ctime = time.time()\n",
    "    print(\"Training User-based Collaborative Filtering...\", )\n",
    "\n",
    "    sim = C.dot(C.T)\n",
    "    norms = [norm(C[i]) for i in range(C.shape[0])]\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "      sim[i][i] = 0.0\n",
    "      for j in range(i+1, C.shape[0]):\n",
    "        sim[i][j] = sim[i][j] / ((norms[i] * norms[j]) + self.epsilon)\n",
    "        sim[j][i] = sim[i][j] / ((norms[i] * norms[j]) + self.epsilon)\n",
    "\n",
    "    self.rec_score = sim.dot(C)\n",
    "    print(\"Done. Elapsed time:\", time.time() - ctime, \"s\")\n",
    "\n",
    "  def predict(self, i, j):\n",
    "    return self.rec_score[i][j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4v6fASRICSsu"
   },
   "source": [
    "## Evaluation process of recommendation (Precision and Recall)\n",
    "\n",
    "In pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Both precision and recall are therefore based on an understanding and measure of relevance. You can read more on [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbaKaLlcCXOT"
   },
   "outputs": [],
   "source": [
    "def precisionk(actual, predicted):\n",
    "  return 1.0 * len(set(actual) & set(predicted)) / len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhjGJcSoCW36"
   },
   "outputs": [],
   "source": [
    "def recallk(actual, predicted):\n",
    "  return 1.0 * len(set(actual) & set(predicted)) / len(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSbZL4luE7uR"
   },
   "source": [
    "## Store the evaluation values\n",
    "\n",
    "In order to store the results of each method based on each users, we have defined some global variables (as [list](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists)) to store each user's precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEXXk7SvE6po"
   },
   "outputs": [],
   "source": [
    "# precison and recall by user-based CF for each user\n",
    "precision_UB, recall_UB = [], []\n",
    "# precison and recall by Poisson Matrix Factorization Matrix Factorization for each user\n",
    "precision_MF, recall_MF = [], []\n",
    "# precison and recall by MF + Geographical Influence for each user\n",
    "precision_MFG, recall_MFG = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C_rhwB3TevOQ"
   },
   "source": [
    "### Recommendation by User-based Collaborative Filtering (UB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n4kTkeEuvWyG"
   },
   "outputs": [],
   "source": [
    "def ub_runer():\n",
    "  top_k = 100\n",
    "  \n",
    "  # remove the last values from pre and rec\n",
    "  del precision_UB[:]\n",
    "  del recall_UB[:]\n",
    "  \n",
    "  U = UserBasedCF()\n",
    "  \n",
    "  # Compute the user-based CF\n",
    "  U.pre_compute_rec_scores(training_matrix)\n",
    "  \n",
    "  all_uids = list(range(n_users))\n",
    "  all_lids = list(range(n_locations))\n",
    "  np.random.shuffle(all_uids)\n",
    "  \n",
    "  # calculate the prediction value for each user on different items\n",
    "  for cnt, uid in enumerate(all_uids):   \n",
    "    if uid in ground_truth:\n",
    "      U_scores = [U.predict(uid, lid)\n",
    "                  if training_matrix[uid, lid] == 0 else -1\n",
    "                  for lid in all_lids]\n",
    "      \n",
    "      U_scores = np.array(U_scores)\n",
    "      \n",
    "      # sort the items based on their prediction value\n",
    "      predicted = list(reversed(U_scores.argsort()))[:top_k]\n",
    "      actual = ground_truth[uid]\n",
    "      \n",
    "      # compute each user's precision and recall\n",
    "      precision_UB.append(precisionk(actual, predicted[:10]))     \n",
    "      recall_UB.append(recallk(actual, predicted[:10]))\n",
    "      \n",
    "  print(\"pre@10:\", np.mean(precision_UB), \"rec@10:\", np.mean(recall_UB))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "YbzOmVD2N6Gj",
    "outputId": "344066ff-1ac9-45b0-9b19-145a2cf05677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training User-based Collaborative Filtering...\n",
      "Done. Elapsed time: 0.02326512336730957 s\n",
      "pre@10: 0.044999999999999984 rec@10: 0.029128869480040422\n"
     ]
    }
   ],
   "source": [
    "# Now, you can run the user-based CF\n",
    "ub_runer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_UKAy0Q9zJG"
   },
   "source": [
    "## Matrix Factorization\n",
    "A technique commonly used for these type of problems is to decompose the matrix of user-location-visits into two low rank matrices, and then compute distances based on the new space. The idea is to take the original visit count matrix, and then reduce that down to two much smaller matrices that approximate the original matrix when multiplied together. You can find more about MF in [quuxlbs blog post](http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/). Here, based on [2], we use a version of matrix factorization called __Poisson Factor Model__, that defines a Poisson distribution on the frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TTfDm-NLFwiG"
   },
   "source": [
    "### Matrix Factorization: Poisson Factor Model [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9NEiQBCFw-6"
   },
   "outputs": [],
   "source": [
    "class PoissonFactorModel(object):\n",
    "  def __init__(self, K=30, alpha=20.0, beta=0.2):\n",
    "    self.K = K\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    self.U, self.L = None, None\n",
    "\n",
    "  def train(self, sparse_check_in_matrix, max_iters=50, learning_rate=1e-4):\n",
    "    ctime = time.time()\n",
    "    print(\"Training PFM...\", )\n",
    "\n",
    "    alpha = self.alpha\n",
    "    beta = self.beta\n",
    "    K = self.K\n",
    "\n",
    "    F = sparse_check_in_matrix\n",
    "    M, N = sparse_check_in_matrix.shape\n",
    "    U = 0.5 * np.sqrt(np.random.gamma(alpha, beta, (M, K))) / K\n",
    "    L = 0.5 * np.sqrt(np.random.gamma(alpha, beta, (N, K))) / K\n",
    "\n",
    "    F = F.tocoo()\n",
    "    entry_index = list(zip(F.row, F.col))\n",
    "\n",
    "    F = F.tocsr()\n",
    "    F_dok = F.todok()\n",
    "\n",
    "    tau = 10\n",
    "    last_loss = float('Inf')\n",
    "    for iters in range(max_iters):\n",
    "      F_Y = F_dok.copy()\n",
    "      for i, j in entry_index:\n",
    "        F_Y[i, j] = 1.0 * F_dok[i, j] / U[i].dot(L[j]) - 1\n",
    "      F_Y = F_Y.tocsr()\n",
    "\n",
    "      learning_rate_k = learning_rate * tau / (tau + iters)\n",
    "      U += learning_rate_k * (F_Y.dot(L) + (alpha - 1) / U - 1 / beta)\n",
    "      L += learning_rate_k * ((F_Y.T).dot(U) + (alpha - 1) / L - 1 / beta)\n",
    "\n",
    "      loss = 0.0\n",
    "      for i, j in entry_index:\n",
    "        loss += (F_dok[i, j] - U[i].dot(L[j]))**2\n",
    "\n",
    "      print('Iteration:', iters,  'loss:', loss)\n",
    "\n",
    "      if loss > last_loss:\n",
    "        print(\"Early termination.\")\n",
    "        break\n",
    "        last_loss = loss\n",
    "\n",
    "      print(\"Done. Elapsed time:\", time.time() - ctime, \"s\")\n",
    "      self.U, self.L = U, L\n",
    "\n",
    "  def predict(self, uid, lid, sigmoid=False):\n",
    "    if sigmoid:\n",
    "      return 1.0 / (1 + math.exp(-self.U[uid].dot(self.L[lid])))\n",
    "    return self.U[uid].dot(self.L[lid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D4jOnzqBYJ2Z"
   },
   "source": [
    "### Recommendation by the Matrix Factoriztion (MF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UeV5UG_5YOx9"
   },
   "outputs": [],
   "source": [
    "def mf_runer():\n",
    "  \n",
    "  top_k = 100\n",
    "  \n",
    "  del precision_MF[:]\n",
    "  del recall_MF[:]\n",
    "  \n",
    "  \n",
    "  PFM = PoissonFactorModel(K=30, alpha=20.0, beta=0.2)\n",
    "  PFM.train(sparse_training_matrix, max_iters=10, learning_rate=1e-4)\n",
    "  \n",
    "  all_uids = list(range(n_users))\n",
    "  all_lids = list(range(n_locations))\n",
    "  np.random.shuffle(all_uids)\n",
    "    \n",
    "  for cnt, uid in enumerate(all_uids):\n",
    "    if (int(cnt) != 0 and int(cnt) % 100 == 0):\n",
    "      print(\"Passed\", int(cnt), \"/\", n_users, \"by Precision@10 and Recall@10: \",  np.mean(precision_MF), \"and\", np.mean(recall_MF))\n",
    "    if uid in ground_truth:\n",
    "      overall_scores = [PFM.predict(uid, lid)\n",
    "                        if training_matrix[uid, lid] == 0 else -1\n",
    "                        for lid in all_lids]     \n",
    "      \n",
    "      overall_scores = np.array(overall_scores)\n",
    "\n",
    "      predicted = list(reversed(overall_scores.argsort()))[:top_k]\n",
    "      actual = ground_truth[uid]\n",
    "      \n",
    "      precision_MF.append(precisionk(actual, predicted[:10]))\n",
    "      recall_MF.append(recallk(actual, predicted[:10]))\n",
    "      \n",
    "  print(\">> Finally Precision@10:\", np.mean(precision_MF), \"Recall@10:\", np.mean(recall_MF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "RvjR4qnjYvs8",
    "outputId": "4c889888-6c98-45b5-ea0b-98c27f133c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PFM...\n",
      "Iteration: 0 loss: 153775.0742121855\n",
      "Done. Elapsed time: 0.4833698272705078 s\n",
      "Iteration: 1 loss: 150364.50931747383\n",
      "Done. Elapsed time: 0.9035508632659912 s\n",
      "Iteration: 2 loss: 147652.94767959445\n",
      "Done. Elapsed time: 1.378556728363037 s\n",
      "Iteration: 3 loss: 145432.11644497717\n",
      "Done. Elapsed time: 1.8299260139465332 s\n",
      "Iteration: 4 loss: 143583.03201941596\n",
      "Done. Elapsed time: 2.2872118949890137 s\n",
      "Iteration: 5 loss: 142026.3093125329\n",
      "Done. Elapsed time: 2.7119579315185547 s\n",
      "Iteration: 6 loss: 140705.05505221526\n",
      "Done. Elapsed time: 3.1376450061798096 s\n",
      "Iteration: 7 loss: 139576.750484202\n",
      "Done. Elapsed time: 3.575303792953491 s\n",
      "Iteration: 8 loss: 138608.704472816\n",
      "Done. Elapsed time: 4.032614707946777 s\n",
      "Iteration: 9 loss: 137775.24553683915\n",
      "Done. Elapsed time: 4.454911947250366 s\n",
      ">> Finally Precision@10: 0.025999999999999995 Recall@10: 0.01728126224118449\n"
     ]
    }
   ],
   "source": [
    "mf_runer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PPm-YQPAY2Ep"
   },
   "source": [
    "## Context Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wf2A4gT99h4Q"
   },
   "source": [
    "One of the most important pieces of context information is the Geographical (or _spatial_) information that shows the users' check-ins behavior. Geographical information allows us to focus recommendations on locations close to the user, keeping our recommendations relevant as a user travels. Users tend to check-in around several centers, where the check-in locations follow a Gaussian distribution at each center as shown in the following piece of code for a typical userâ€™s check-in behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "colab_type": "code",
    "id": "excCJaGeo9hB",
    "outputId": "e14029c7-9b94-4766-af84-dd421d3da82f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <script>L_PREFER_CANVAS=false; L_NO_TOUCH=false; L_DISABLE_3D=false;</script>
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.3.4/dist/leaflet.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.3.4/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
    <meta name="viewport" content="width=device-width,
        initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <style>#map_d6cb58be94bb430b9ff783412893fbfb {
        position: relative;
        width: 100.0%;
        height: 100.0%;
        left: 0.0%;
        top: 0.0%;
        }
    </style>
</head>
<body>    
    
    <div class="folium-map" id="map_d6cb58be94bb430b9ff783412893fbfb" ></div>
</body>
<script>    
    
    
        var bounds = null;
    

    var map_d6cb58be94bb430b9ff783412893fbfb = L.map(
        'map_d6cb58be94bb430b9ff783412893fbfb', {
        center: [30.2691029532, -97.7493953705],
        zoom: 5,
        maxBounds: bounds,
        layers: [],
        worldCopyJump: false,
        crs: L.CRS.EPSG3857,
        zoomControl: true,
        });

    
    
    var tile_layer_4df17c5190cc45dea59563165161c1e9 = L.tileLayer(
        'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png',
        {
        "attribution": null,
        "detectRetina": false,
        "maxNativeZoom": 18,
        "maxZoom": 18,
        "minZoom": 0,
        "noWrap": false,
        "opacity": 1,
        "subdomains": "abc",
        "tms": false
}).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
    
        var marker_2e8edcfa8668455aa8ef10894beab25c = L.marker(
            [30.2691029532, -97.7493953705],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_f36f88595d9f4d55849310c521e2ddf7 = L.marker(
            [30.2557309927, -97.7633857727],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_473af730a0894e8cb6efff50557721ef = L.marker(
            [30.2679095833, -97.74931241670001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_866959cdd74a419a851b2fb34783062e = L.marker(
            [30.2811204101, -97.7452111244],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_3bc71000a1d84b09bf375a1731e1eb0c = L.marker(
            [30.2448597206, -97.7571630478],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_952adf82c6b240d6b2207eea4eb3f77c = L.marker(
            [30.3170157639, -97.7195692062],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_62700837a598454b9f674a6a5e121bf3 = L.marker(
            [30.265405690999998, -97.74779140950001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_21a5732fcfca4f82ac7404ec8c2fc73c = L.marker(
            [30.2464607667, -97.7507881833],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_396f93848f244772b737be45a164937c = L.marker(
            [38.9927760833, -94.5952756],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_c6e8f3e7f26e4501ab9bf56b08241c1e = L.marker(
            [39.041052948899996, -94.5947393775],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_1cd1183294c04ed996afbde7f2594942 = L.marker(
            [39.0528237667, -94.59031105],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_a468b741bcb04d30aea3fa4a5894eca5 = L.marker(
            [38.934693546, -94.62979316709999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_f9166039579f49409322c1d989ead7d3 = L.marker(
            [39.057333533299996, -94.60617950000001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_b50db5088fd74f0aa378fa0441233416 = L.marker(
            [39.0506711667, -94.5980687833],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_bccf571228ef455880a1595b01d7fde7 = L.marker(
            [37.7662367387, -96.98147892950001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_2423eb329bb84a42af05218b0cbcac44 = L.marker(
            [34.185646033299996, -97.1656978333],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_c3b1cba0f4944b6fb7788c25ff62e9d1 = L.marker(
            [30.2637747742, -97.77065992360001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_de85c05de37d42de96b85e8515db4c46 = L.marker(
            [30.2668906357, -97.74568587540001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_7460b6e572264459b1e57483f88bfe8c = L.marker(
            [30.2686964002, -97.7456147969],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_af63ab79afd64079a179611c410f71da = L.marker(
            [30.310152662199997, -97.7401578426],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_89acf89ea38841cdaf9f6d23b414ef37 = L.marker(
            [30.26485415, -97.7438453833],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_d57783d738024842b2e90eb9f80d967a = L.marker(
            [30.2509939667, -97.7489662167],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_85d93d6757eb4656b7730b4a8305b5bf = L.marker(
            [30.310959277, -97.74269296],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_8b15dcca19a24056a569ca57edab07e1 = L.marker(
            [30.31104325, -97.74291515],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_9da3a503d32f4d6aac480b0a128f030e = L.marker(
            [30.2662396721, -97.74590849879999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_1142dfb884754ef9bab3ae8539d6df65 = L.marker(
            [30.270733781599997, -97.7537029982],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_6f68adf506d44ec3a8575f00f4543d5b = L.marker(
            [30.201557329699998, -97.6671266556],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_359c62118ec94fd08b1adb3dd8a941d0 = L.marker(
            [37.616356064899996, -122.38615036],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_9776fec8b5ac4ded9aa7010c9770e5ea = L.marker(
            [37.7615082532, -122.42576658700001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_65cc3d75563246548eec83e30edab764 = L.marker(
            [37.759688872, -122.427177429],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_c1a6d8b2dc224615805a6e7b0c2637ae = L.marker(
            [37.78594785, -122.41061780000001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_13c378930cfa4892a83d6718a0400b04 = L.marker(
            [37.4191867667, -122.21220151700001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_cb3c4a74450542bc8eeda2f462b52eef = L.marker(
            [37.4157602871, -122.152551413],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_57d6abfe3a04496fbc2cd8dc8073d136 = L.marker(
            [37.7826046833, -122.407608017],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_0ac88cf91dca41a99be89a3235668f3b = L.marker(
            [30.2702114098, -97.7497132123],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_9ef8ca1bf5e84b63a2439122e2aa6687 = L.marker(
            [30.2493640765, -97.74947047229999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_a3df4d75c6f042528eb317c55a85b37e = L.marker(
            [30.2450661333, -97.75152633329999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_0ed36823558c4626beb99fa0272086a6 = L.marker(
            [30.2642977167, -97.73298025],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_bbedfc14b3554829b46e383e35af6c0b = L.marker(
            [30.272031012, -97.75411069389999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_e00dfa75d82749c8ad44b1df4dd1355b = L.marker(
            [30.251046205799998, -97.74932429190001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_7ca290651bbf44679bc8173d63cfd94d = L.marker(
            [30.2690658886, -97.7456724644],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_4ad3813e7a3f4f56978e366c7d52d349 = L.marker(
            [30.2515346167, -97.7490263167],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_8cdb9f46df164996ba79a74262409a8d = L.marker(
            [30.2510314833, -97.754092],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_4178bd0a0f54464798beef7782963b32 = L.marker(
            [39.849501881799995, -104.673871994],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_d05f5938487d4809a6096ca7c39301d7 = L.marker(
            [39.2191109667, -106.86401495],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_a654254ab7a64050a2dcd673287c6407 = L.marker(
            [30.254141466700002, -97.7623172167],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_566b9f85aee948588e9007c90488c2e3 = L.marker(
            [30.2541434833, -97.7623320667],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_f24fba3e9f3a499198db79c065c2e719 = L.marker(
            [30.3307521333, -97.73175715],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_11309e819ac74954866125a2df1a6b2e = L.marker(
            [30.26406815, -97.7672601333],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_bb9697769cf94bfbac8b6906d1b1ebeb = L.marker(
            [30.2506754911, -97.7495348454],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_6b61b5e6c06d4beba8db55bb3efe2185 = L.marker(
            [30.263010098000002, -97.72508164610001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_570b94195c10400cb3ed73e61dcf50be = L.marker(
            [30.269767796999997, -97.7482661605],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_737f41ee6ec1434bb6e3f9a1e0e04e37 = L.marker(
            [30.2627183704, -97.75037169459999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_340b0d2ffb0f48dfb81d24fbcb734ce3 = L.marker(
            [30.2545781333, -97.769175],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_19941303a7244a6d887b5ecb3ed56b70 = L.marker(
            [30.283552445799998, -97.7225679159],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_f9f343b94be549b5bd210a8315d33eeb = L.marker(
            [30.27055195, -97.7483066333],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_36460899c62041ac95249688bac53511 = L.marker(
            [30.2638859739, -97.7379798889],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_cf1fc2ca9a7d4e2fb0f6e794fdd31c9a = L.marker(
            [30.26175155, -97.7618140833],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_fbd19b4012474096ba8e500991c18032 = L.marker(
            [37.5370606667, -122.327889383],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_4d4b9bf2831247f4bb2f63cd9111c647 = L.marker(
            [30.2725313677, -97.7536225319],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_d13a319c029c4590b92f05248f97403a = L.marker(
            [30.26280055, -97.7250684167],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_bb3f59b4cca747c2a15c1308ffb1c0b1 = L.marker(
            [38.9989524833, -94.5939345333],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_89ba454e8b0749c19ea1486fe3013034 = L.marker(
            [39.0369789909, -94.58718672149999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_4beaa2a67b574f61a1f6bac09596c2fe = L.marker(
            [37.783129592399995, -122.40387439700001],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_d9d0894809534eeb9c92203f4c922510 = L.marker(
            [37.7864788714, -122.403563261],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_366a624bc561415aafecc92108144d33 = L.marker(
            [37.779557700000005, -122.39790915],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_2a2fda2b64144cad8ccd3d8807209495 = L.marker(
            [37.7825583, -122.400125433],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_8ce0fda564b245a096cd945404b65651 = L.marker(
            [37.7857173715, -122.399246205],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_3cdb505707f64313a0ba39ad2f3a20f3 = L.marker(
            [37.7881916086, -122.401213646],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_907d2577cbad4602becda6cfe9946572 = L.marker(
            [37.7614446392, -122.423958778],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_a59a8b96eb2c48ac875a26d6f32ddaef = L.marker(
            [37.7854359476, -122.40394949899999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_4fab74b96723436da15fb88dff4dd617 = L.marker(
            [37.7815086, -122.405028233],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_b6cd72f478fe47d5be227e533c47b4c4 = L.marker(
            [32.8974616458, -97.04034805299999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_f850f073a03b42b69559b013044ee5be = L.marker(
            [30.2624620667, -97.7623715333],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_bba8494d662140e7b4c81b281258b590 = L.marker(
            [30.2794862333, -97.7599647667],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_0ccd62fa967a4a7881d70f5a509306dd = L.marker(
            [30.213861189299998, -97.7694153786],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_8460291e598d43c2bbbddbea267ed3b5 = L.marker(
            [37.7953388902, -122.39372491799999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_e29a2dd9ab7a4897810c454aef7f5b86 = L.marker(
            [39.0935326667, -94.5931743],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_a6004facc72c4a259f896847b75bb457 = L.marker(
            [39.2974431129, -94.71605300899999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
    
        var marker_2bd3ce64605348409da3277a46b39409 = L.marker(
            [30.248923845, -97.74962604049999],
            {
                icon: new L.Icon.Default()
                }
            ).addTo(map_d6cb58be94bb430b9ff783412893fbfb);
        
</script>\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x10db175c0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which user's check-ins distribution want to show?\n",
    "which_user = 0\n",
    "\n",
    "locationlist = []\n",
    "# get the locations that have been checked-in by which_user\n",
    "for user_checkins in range(training_matrix.shape[0]):\n",
    "  if user_checkins == which_user:\n",
    "    locations = training_matrix[user_checkins].nonzero()\n",
    "  break\n",
    "  \n",
    "for locs in locations:\n",
    "  for loc in locs:\n",
    "    lat, lng = poi_coos[loc]\n",
    "    # Make a list with the coordinates of each location\n",
    "    locationlist.append([lat, lng])\n",
    "\n",
    "# Open a map and initialize it with the coordinates\n",
    "map = folium.Map(location=locationlist[0], zoom_start=5)\n",
    "# add to the map all different locations\n",
    "for point in range(0, len(locationlist)):\n",
    "    folium.Marker(locationlist[point]).add_to(map)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iarhCXloPGqQ"
   },
   "source": [
    "### Geographical Information: Multi-center Gaussian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttZljKD5_C_A"
   },
   "source": [
    "Here, we consider embedding the geographical influence based on the idea mentioned above. Then, we fused the geographical influence into our MF method to show the impact of contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cM_EmZK2YKZ4"
   },
   "source": [
    "#### Find the distance between to locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZfRRK_JYIH8"
   },
   "outputs": [],
   "source": [
    "def dist(loc1, loc2):\n",
    "    lat1, long1 = loc1.lat, loc1.lng\n",
    "    lat2, long2 = loc2.lat, loc2.lng\n",
    "    if abs(lat1 - lat2) < 1e-6 and abs(long1 - long2) < 1e-6:\n",
    "        return 0.0\n",
    "    degrees_to_radians = math.pi/180.0\n",
    "    phi1 = (90.0 - lat1)*degrees_to_radians\n",
    "    phi2 = (90.0 - lat2)*degrees_to_radians\n",
    "    theta1 = long1*degrees_to_radians\n",
    "    theta2 = long2*degrees_to_radians\n",
    "    cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) +\n",
    "           math.cos(phi1)*math.cos(phi2))\n",
    "    arc = math.acos(cos)\n",
    "    earth_radius = 6371\n",
    "    return arc * earth_radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JUtm3ohYQTI"
   },
   "source": [
    "#### The geographical approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9BYmxP8yFYdh"
   },
   "outputs": [],
   "source": [
    "class Location(object):\n",
    "    def __init__(self, id, lat, lng, freq, center=-1):\n",
    "        self.id = id\n",
    "        self.lat = lat\n",
    "        self.lng = lng\n",
    "        self.freq = freq\n",
    "        self.center = center\n",
    "\n",
    "\n",
    "class Center(object):\n",
    "    def __init__(self):\n",
    "        self.locations = []\n",
    "        self.total_freq = 0\n",
    "        self.distribution = None\n",
    "        self.mu = None\n",
    "        self.cov = None\n",
    "        self.lat = None\n",
    "        self.lng = None\n",
    "\n",
    "    def add(self, loc):\n",
    "        self.locations.append(loc)\n",
    "        self.total_freq += loc.freq\n",
    "\n",
    "    def build_gaussian(self):\n",
    "        coo_seq = []\n",
    "        for loc in self.locations:\n",
    "            for _ in range(int(loc.freq)):\n",
    "                coo_seq.append(np.array([loc.lat, loc.lng]))\n",
    "        coo_seq = np.array(coo_seq)\n",
    "        self.mu = np.mean(coo_seq, axis=0)\n",
    "        self.cov = np.cov(coo_seq.T)\n",
    "        self.distribution = multivariate_normal(self.mu, self.cov, allow_singular=True)\n",
    "        self.lat = self.mu[0]\n",
    "        self.lng = self.mu[1]\n",
    "\n",
    "    def pdf(self, x):\n",
    "        return self.distribution.pdf(np.array([x.lat, x.lng]))\n",
    "\n",
    "\n",
    "class MultiGaussianModel(object):\n",
    "    def __init__(self, alpha=0.2, theta=0.02, dmax=15):\n",
    "        self.alpha = alpha\n",
    "        self.theta = theta\n",
    "        self.dmax = dmax\n",
    "        self.poi_coos = None\n",
    "        self.center_list = None\n",
    "\n",
    "    def build_user_check_in_profile(self, sparse_check_in_matrix):\n",
    "        L = defaultdict(list)\n",
    "        for (uid, lid), freq in sparse_check_in_matrix.items():\n",
    "            lat, lng = self.poi_coos[lid]\n",
    "            L[uid].append(Location(lid, lat, lng, freq))\n",
    "        return L\n",
    "\n",
    "    def discover_user_centers(self, Lu):\n",
    "        center_min_freq = max(sum([loc.freq for loc in Lu]) * self.theta, 2)\n",
    "        Lu.sort(key=lambda k: k.freq, reverse=True)\n",
    "        center_list = []\n",
    "        center_num = 0\n",
    "        for i in range(len(Lu)):\n",
    "            if Lu[i].center == -1:\n",
    "                center_num += 1\n",
    "                center = Center()\n",
    "                center.add(Lu[i])\n",
    "                Lu[i].center = center_num\n",
    "                for j in range(i+1, len(Lu)):\n",
    "                    if Lu[j].center == -1 and dist(Lu[i], Lu[j]) <= self.dmax:\n",
    "                        Lu[j].center = center_num\n",
    "                        center.add(Lu[j])\n",
    "                if center.total_freq >= center_min_freq:\n",
    "                    center_list.append(center)\n",
    "        return center_list\n",
    "\n",
    "    def multi_center_discovering(self, sparse_check_in_matrix, poi_coos):\n",
    "        self.poi_coos = poi_coos\n",
    "        L = self.build_user_check_in_profile(sparse_check_in_matrix)\n",
    "\n",
    "        center_list = {}\n",
    "        for uid in range(len(L)):\n",
    "            center_list[uid] = self.discover_user_centers(L[uid])\n",
    "            for cid in range(len(center_list[uid])):\n",
    "                center_list[uid][cid].build_gaussian()\n",
    "        self.center_list = center_list\n",
    "\n",
    "    def predict(self, uid, lid):\n",
    "        lat, lng = self.poi_coos[lid]\n",
    "        l = Location(None, lat, lng, None)\n",
    "\n",
    "        prob = 0.0\n",
    "        if uid in self.center_list:\n",
    "            all_center_freq = sum([cid.total_freq**self.alpha for cid in self.center_list[uid]])\n",
    "            all_center_pdf = sum([cid.pdf(l) for cid in self.center_list[uid]])\n",
    "            if not all_center_pdf == 0:\n",
    "                for cu in self.center_list[uid]:\n",
    "                    prob += (\n",
    "                        1.0 / (dist(l, cu) + 1) *\n",
    "                        (cu.total_freq**self.alpha) / all_center_freq *\n",
    "                        cu.pdf(l) / all_center_pdf)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWf3nafNF2QV"
   },
   "source": [
    "### Run the Matrix Factorization with Multi-Center Geographical Model (MFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6jEjVD33iXiw"
   },
   "outputs": [],
   "source": [
    "def mfg_runer():\n",
    "  \n",
    "  top_k = 100\n",
    "  \n",
    "  del precision_MFG[:]\n",
    "  del recall_MFG[:]\n",
    "  \n",
    "  PFM = PoissonFactorModel(K=30, alpha=20.0, beta=0.2)\n",
    "  PFM.train(sparse_training_matrix, max_iters=10, learning_rate=1e-4)\n",
    "  \n",
    "  print(\"Starting Multi-Center Gaussian Model ...\")\n",
    "  MGM = MultiGaussianModel(alpha=0.2, theta=0.02, dmax=15)\n",
    "  MGM.multi_center_discovering(sparse_training_matrix, poi_coos)\n",
    "  print(\"Done Multi-Center Gaussian Model.\")\n",
    "  \n",
    "  all_uids = list(range(n_users))\n",
    "  all_lids = list(range(n_locations))\n",
    "  np.random.shuffle(all_uids)\n",
    "    \n",
    "  for cnt, uid in enumerate(all_uids):\n",
    "    if (int(cnt) != 0 and int(cnt) % 100 == 0):\n",
    "      print(\"Passed\", int(cnt), \"/\", n_users, \"by Precision@10 and Recall@10: \",  np.mean(precision_MFG), \"and\", np.mean(recall_MFG))\n",
    "    if uid in ground_truth:\n",
    "      overall_scores = [PFM.predict(uid, lid) * MGM.predict(uid, lid)\n",
    "                        if training_matrix[uid, lid] == 0 else -1\n",
    "                        for lid in all_lids]     \n",
    "      \n",
    "      overall_scores = np.array(overall_scores)\n",
    "\n",
    "      predicted = list(reversed(overall_scores.argsort()))[:top_k]\n",
    "      actual = ground_truth[uid]\n",
    "      \n",
    "      precision_MFG.append(precisionk(actual, predicted[:10]))\n",
    "      recall_MFG.append(recallk(actual, predicted[:10]))\n",
    "      \n",
    "  print(\">> Finally Precision@10:\", np.mean(precision_MFG), \"Recall@10:\", np.mean(recall_MFG))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "1pQcyEwaifTa",
    "outputId": "95073887-3aec-4409-d4f8-d83fc1380aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PFM...\n",
      "Iteration: 0 loss: 153748.44272700133\n",
      "Done. Elapsed time: 0.5810122489929199 s\n",
      "Iteration: 1 loss: 150340.87873130923\n",
      "Done. Elapsed time: 0.9840230941772461 s\n",
      "Iteration: 2 loss: 147631.99659596282\n",
      "Done. Elapsed time: 1.404954195022583 s\n",
      "Iteration: 3 loss: 145413.5430163416\n",
      "Done. Elapsed time: 1.8383781909942627 s\n",
      "Iteration: 4 loss: 143566.57569360192\n",
      "Done. Elapsed time: 2.2838010787963867 s\n",
      "Iteration: 5 loss: 142011.74730569738\n",
      "Done. Elapsed time: 2.7031233310699463 s\n",
      "Iteration: 6 loss: 140692.19642876455\n",
      "Done. Elapsed time: 3.1351821422576904 s\n",
      "Iteration: 7 loss: 139565.43074305367\n",
      "Done. Elapsed time: 3.594578266143799 s\n",
      "Iteration: 8 loss: 138598.78103972494\n",
      "Done. Elapsed time: 4.086395263671875 s\n",
      "Iteration: 9 loss: 137766.59410123777\n",
      "Done. Elapsed time: 4.612605333328247 s\n",
      "Starting Multi-Center Gaussian Model ...\n",
      "Done Multi-Center Gaussian Model.\n"
     ]
    }
   ],
   "source": [
    "mfg_runer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xh4GBlODFf0l"
   },
   "source": [
    "## Expriment Comparsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "SnTpHs2SCizH",
    "outputId": "00749776-222d-4273-962d-c89d5017c391"
   },
   "outputs": [],
   "source": [
    "# data to plot\n",
    "n_groups = 3\n",
    "means_precision = (np.mean(precision_UB), np.mean(precision_MF), np.mean(precision_MFG))\n",
    "means_recall = (np.mean(recall_UB), np.mean(recall_MF), np.mean(recall_MFG))\n",
    "\n",
    " \n",
    "# create plot\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.25\n",
    "opacity = 0.8\n",
    " \n",
    "rects1 = plt.bar(index, means_precision, bar_width,\n",
    "alpha=opacity,\n",
    "label='Precision')\n",
    " \n",
    "rects2 = plt.bar(index + bar_width, means_recall, bar_width,\n",
    "alpha=opacity,\n",
    "label='Recall')\n",
    " \n",
    "plt.xlabel('Methods')\n",
    "plt.ylabel('Precision and Recall')\n",
    "plt.title('Results')\n",
    "plt.xticks(index + 0.1, ('UB', 'MF', 'MFG'))\n",
    "plt.legend()\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ft3GCk70A5Y5"
   },
   "source": [
    "### Significant results\n",
    "You see some difference in the bar chart above but to show whether the difference in results achieved is significant or not (i.e. there is a significant difference between the means of two distributions) you can use the _t-Test_. For this purpose, you need to run _t-Test_ for each result based on each metric. You can find more details in the following link and see how to use _t-Test_ in python with Scipy: [t-Test in Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XKM8x0e9vZW"
   },
   "source": [
    "### Conclusion\n",
    "As you can see in the plot, UB method is outperforming other methods. Not achieveing better performance form MF based methods can be due to the inadequate size of the datatset used. The _user-based_ approach uses all of users' and items' information being a memory-based approach. Here, the dataset is very small, we only have considered data of 100 users. When the number of users and items grow, the MF approaches will show their performance gain better. We can also see that MFG is outperforming MF. This shows the impact of Context information in modeling users' behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-evygSNlY2XX"
   },
   "source": [
    "## Acknowledge\n",
    "For implemenation we got some information and useful review of the codes that provided by the following paper:  \n",
    "__Liu, Yiding, et al. \"An experimental evaluation of point-of-interest recommendation in location-based social networks.\" in VLDB, 2017__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJUOWKG0UdTI"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] __Ye, Mao, et al. \"Exploiting geographical influence for collaborative point-of-interest recommendation\" in SIGIR, 2011.__  \n",
    "[2] __Cheng, Chen, et al. \"Fused matrix factorization with geographical and social influence in location-based social networks\", in AAAI, 2012.__"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "POI Recommender Systems.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
